{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsv9eEEeK0xS"
      },
      "source": [
        "# Genshin YOLO detection Training Notebook\n",
        "\n",
        "In this notebook, you will see how we train the Ultralytics YOLOv11 model to detect the most important elements visible in Genshin Impact. _(This notebook is designed for Google Colab but can be run anywhere with some adjustments)_\n",
        "\n",
        "**First, let's verify that we have a GPU active**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2oqyJgkBXC-"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use YOLOv11 (by Ultralytics), so we have to **install Ultralytics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lNYw-7LA-fP"
      },
      "source": [
        "# Get the data set from Github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7i5yN6UM9HE"
      },
      "outputs": [],
      "source": [
        "!rm -rf dataset\n",
        "!git clone https://github.com/baptdes/Genshin-YOLO-Detection.git\n",
        "!mv Genshin-YOLO-Detection/dataset dataset\n",
        "!rm -rf Genshin-YOLO-Detection\n",
        "\n",
        "DATASET_PATH = '/content/dataset'\n",
        "DATA_PATH = '/content/data.yaml'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create data.yaml\n",
        "\n",
        "As we used LabelStudio to labelise our DataSet, we have a file ```classes.txt``` in our dataset and not a file ```data.yaml``` that is necessary to train our YOLO model. So, **let's create the file ```data.yaml```** using the file ```classes.txt```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "def create_data_yaml(path_to_classes_txt, path_to_data_yaml):\n",
        "\n",
        "  # Read class.txt to get class names and index\n",
        "  if not os.path.exists(path_to_classes_txt):\n",
        "    print(f'classes.txt file not found! Please create a classes.txt labelmap and move it to {path_to_classes_txt}')\n",
        "    return\n",
        "  with open(path_to_classes_txt, 'r') as f:\n",
        "    classes = []\n",
        "    for line in f.readlines():\n",
        "      if len(line.strip()) == 0: continue\n",
        "      classes.append(line.strip())\n",
        "  number_of_classes = len(classes)\n",
        "\n",
        "  # Write data to YAML file\n",
        "  data = {\n",
        "      'nc': number_of_classes,\n",
        "      'names': classes\n",
        "  }\n",
        "  with open(path_to_data_yaml, 'w') as f:\n",
        "    yaml.dump(data, f, sort_keys=False)\n",
        "  print(f'Created config file at {path_to_data_yaml}')\n",
        "\n",
        "  return\n",
        "\n",
        "# Define path to classes.txt and run function\n",
        "path_to_classes_txt = '/content/dataset/classes.txt'\n",
        "path_to_data_yaml = '/content/data.yaml'\n",
        "\n",
        "create_data_yaml(path_to_classes_txt, path_to_data_yaml)\n",
        "\n",
        "print('\\nFile contents:\\n')\n",
        "!cat /content/data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split the data set\n",
        "\n",
        "Now that we have our data and classes, we can split our data set into 3 subsets :\n",
        "* Training set : Used to train the model.\n",
        "* Validation set : Used to tune hyperparameters and prevent overfitting.\n",
        "* Test set : Used to evaluate the final performance of the model.\n",
        "\n",
        "For now, we are doing a random split to do that, but we are thinking of doing a stratified split based on this [article](https://jaidevd.com/posts/obj-detection-stratification/)\n",
        "\n",
        "**First, we need to create a feature vector for each image to track the number of instances of each class present in the image**\n",
        "\n",
        "## Feature Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLd5aZAPDyvm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========================== Load Dataset and YAML ==================================\n",
        "dataset_path = Path(DATASET_PATH)\n",
        "labels = sorted(dataset_path.rglob(\"labels/*.txt\"))  # All label files\n",
        "\n",
        "yaml_file = DATA_PATH\n",
        "with open(yaml_file, encoding=\"utf8\") as y:\n",
        "    classes = yaml.safe_load(y)[\"names\"]  # YOLO class names\n",
        "\n",
        "cls_idx = list(range(len(classes)))  # Class indices\n",
        "\n",
        "# ============== Create Feature Vectors (Number of classes per image) ===============\n",
        "index = [label.stem for label in labels]  # Use filenames (without extension) as IDs\n",
        "labels_df = pd.DataFrame([], columns=cls_idx, index=index)\n",
        "\n",
        "for label in labels:\n",
        "    lbl_counter = Counter()\n",
        "\n",
        "    with open(label) as lf:\n",
        "        lines = lf.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        lbl_counter[int(line.split(\" \")[0])] += 1  # Extract YOLO class index\n",
        "\n",
        "    labels_df.loc[label.stem] = lbl_counter\n",
        "\n",
        "labels_df = labels_df.fillna(0).infer_objects(copy=False).astype(int)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class_instances = labels_df.sum(axis=0)\n",
        "class_names = [classes[idx] for idx in class_instances.index]\n",
        "\n",
        "# Plot horizontal bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(class_names, class_instances, color=\"skyblue\")\n",
        "plt.xlabel(\"Number of Instances\")\n",
        "plt.ylabel(\"Class Names\")\n",
        "plt.title(\"Instances per Class\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_c-3zbZETne"
      },
      "source": [
        "Now, we can split the dataset using these feature vectors\n",
        "## Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================ Random Split ==================================\n",
        "validation_size = 0.15\n",
        "test_size = 0.10\n",
        "\n",
        "\n",
        "train_files, temp_files = train_test_split(\n",
        "    labels_df.index, test_size= validation_size + test_size, random_state=42\n",
        ")\n",
        "\n",
        "test_size = test_size / (validation_size + test_size)\n",
        "val_files, test_files = train_test_split(\n",
        "    temp_files, test_size=test_size, random_state=42\n",
        ")\n",
        "\n",
        "# ======================= Create output folder =============================\n",
        "output_path = Path(\"split_dataset\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    (output_path / split / \"images\").mkdir(parents=True, exist_ok=True)\n",
        "    (output_path / split / \"labels\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def move_files(files, split):\n",
        "    for file in files:\n",
        "        src_img = dataset_path / f\"images/{file}.png\"\n",
        "        src_lbl = dataset_path / f\"labels/{file}.txt\"\n",
        "\n",
        "        dst_img = output_path / split / \"images\" / f\"{file}.png\"\n",
        "        dst_lbl = output_path / split / \"labels\" / f\"{file}.txt\"\n",
        "\n",
        "        if src_img.exists():\n",
        "            shutil.copy(src_img, dst_img)\n",
        "        if src_lbl.exists():\n",
        "            shutil.copy(src_lbl, dst_lbl)\n",
        "\n",
        "move_files(train_files, \"train\")\n",
        "move_files(val_files, \"val\")\n",
        "move_files(test_files, \"test\")\n",
        "\n",
        "# ======================= Create data_split.yaml =============================\n",
        "yaml_content = {\n",
        "    \"train\": str(output_path / \"train\"),\n",
        "    \"val\": str(output_path / \"val\"),\n",
        "    \"test\": str(output_path / \"test\"),\n",
        "    \"nc\": len(classes),\n",
        "    \"names\": classes,\n",
        "}\n",
        "\n",
        "with open(output_path / \"data_split.yaml\", \"w\") as f:\n",
        "    yaml.dump(yaml_content, f)\n",
        "\n",
        "print(\"Dataset successfully split into train, val, and test with stratified distribution.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split distributions\n",
        "\n",
        "Is our split good ? Let's see that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================== Plot Split Distributions =====================\n",
        "import numpy as np\n",
        "\n",
        "# Function to compute class densities (proportions)\n",
        "def compute_class_densities(file_list):\n",
        "    subset_df = labels_df.loc[file_list]  # Filter dataset for the split\n",
        "    class_counts = subset_df.sum(axis=0)  # Get total instances per class\n",
        "    total_instances = class_counts.sum()  # Sum across all classes\n",
        "\n",
        "    return class_counts / total_instances if total_instances > 0 else class_counts  # Normalize\n",
        "\n",
        "# Compute densities for each split\n",
        "train_densities = compute_class_densities(train_files)\n",
        "val_densities = compute_class_densities(val_files)\n",
        "test_densities = compute_class_densities(test_files)\n",
        "\n",
        "# Create a horizontal bar chart with normalized densities\n",
        "y = np.arange(len(classes))  # Class indices\n",
        "width = 0.25  # Bar width\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot the bars horizontally\n",
        "plt.barh(y - width, train_densities, height=width, label=\"Train\", color=\"blue\", alpha=0.7)\n",
        "plt.barh(y, val_densities, height=width, label=\"Validation\", color=\"orange\", alpha=0.7)\n",
        "plt.barh(y + width, test_densities, height=width, label=\"Test\", color=\"green\", alpha=0.7)\n",
        "\n",
        "# Add labels and title\n",
        "plt.ylabel(\"Class Names\")\n",
        "plt.xlabel(\"Proportion\")\n",
        "plt.title(\"Normalized Class Distribution in Train, Validation, and Test Sets\")\n",
        "plt.yticks(ticks=y, labels=classes)  # Set class names on the y-axis\n",
        "plt.legend()\n",
        "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Old code for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0QYOQK0EY91"
      },
      "outputs": [],
      "source": [
        "!yolo detect train data=/content/data.yaml model=yolo11s.pt epochs=60 imgsz=640"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yKEV0EWFdGt"
      },
      "source": [
        "Testing with the validation folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yBhUvqqFi_1"
      },
      "outputs": [],
      "source": [
        "!yolo detect predict model=runs/detect/train/weights/best.pt source=data/validation/images save=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P20mQm6MFus3"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "for image_path in glob.glob(f'/content/runs/detect/predict2/*.jpg')[:10]:\n",
        "  display(Image(filename=image_path, height=400))\n",
        "  print('\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
