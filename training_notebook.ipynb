{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsv9eEEeK0xS"
      },
      "source": [
        "# Genshin YOLO detection Training Notebook\n",
        "\n",
        "In this notebook, you will see how we train the Ultralytics YOLOv11 model to detect the most important elements visible in Genshin Impact. _(This notebook is designed for Google Colab but can be run anywhere with some adjustments)_\n",
        "\n",
        "**First, let's verify that we have a GPU active**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2oqyJgkBXC-"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq2mcIIMMbXh"
      },
      "source": [
        "We will use YOLOv11 (by Ultralytics), so we have to **install Ultralytics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-C9sBWRMbXi"
      },
      "outputs": [],
      "source": [
        "!pip install -q ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lNYw-7LA-fP"
      },
      "source": [
        "# Get the data set from Github"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y megatools\n",
        "!megadl 'https://mega.nz/file/vRlEEQRA#lNBvZqg_MUG5MBn6XuvPb-za53pwwV6IuIu5guMicic'"
      ],
      "metadata": {
        "id": "6p5C7Kpm1n72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7i5yN6UM9HE"
      },
      "outputs": [],
      "source": [
        "!unzip -q genshin-YOLO-database-uncompressed.zip -d /content/dataset\n",
        "\n",
        "DATASET_PATH = '/content/dataset'\n",
        "DATA_PATH = '/content/data.yaml'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-5coy0xMbXj"
      },
      "source": [
        "# Create data.yaml\n",
        "\n",
        "As we used LabelStudio to labelise our DataSet, we have a file ```classes.txt``` in our dataset and not a file ```data.yaml``` that is necessary to train our YOLO model. So, **let's create the file ```data.yaml```** using the file ```classes.txt```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0b42Jp-MbXk"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "def create_data_yaml(path_to_classes_txt, path_to_data_yaml):\n",
        "\n",
        "  # Read class.txt to get class names and index\n",
        "  if not os.path.exists(path_to_classes_txt):\n",
        "    print(f'classes.txt file not found! Please create a classes.txt labelmap and move it to {path_to_classes_txt}')\n",
        "    return\n",
        "  with open(path_to_classes_txt, 'r') as f:\n",
        "    classes = []\n",
        "    for line in f.readlines():\n",
        "      if len(line.strip()) == 0: continue\n",
        "      classes.append(line.strip())\n",
        "  number_of_classes = len(classes)\n",
        "\n",
        "  # Write data to YAML file\n",
        "  data = {\n",
        "      'nc': number_of_classes,\n",
        "      'names': classes\n",
        "  }\n",
        "  with open(path_to_data_yaml, 'w') as f:\n",
        "    yaml.dump(data, f, sort_keys=False)\n",
        "  print(f'Created config file at {path_to_data_yaml}')\n",
        "\n",
        "  return\n",
        "\n",
        "# Define path to classes.txt and run function\n",
        "path_to_classes_txt = '/content/dataset/classes.txt'\n",
        "path_to_data_yaml = '/content/data.yaml'\n",
        "\n",
        "create_data_yaml(path_to_classes_txt, path_to_data_yaml)\n",
        "\n",
        "print('\\nFile contents:\\n')\n",
        "!cat /content/data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "335a1Q7vMbXm"
      },
      "source": [
        "# Split the data set\n",
        "\n",
        "Now that we have our data and classes, we can split our data set into 3 subsets :\n",
        "* Training set : Used to train the model.\n",
        "* Validation set : Used to tune hyperparameters and prevent overfitting.\n",
        "* Test set : Used to evaluate the final performance of the model.\n",
        "\n",
        "For now, we are doing a random split to do that, but we are thinking of doing a stratified split based on this [article](https://jaidevd.com/posts/obj-detection-stratification/)\n",
        "\n",
        "**First, we need to create a feature vector for each image to track the number of instances of each class present in the image**\n",
        "\n",
        "## Feature Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLd5aZAPDyvm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========================== Load Dataset and YAML ==================================\n",
        "dataset_path = Path(DATASET_PATH)\n",
        "labels = sorted(dataset_path.rglob(\"labels/*.txt\"))  # All label files\n",
        "\n",
        "yaml_file = DATA_PATH\n",
        "with open(yaml_file, encoding=\"utf8\") as y:\n",
        "    classes = yaml.safe_load(y)[\"names\"]  # YOLO class names\n",
        "\n",
        "cls_idx = list(range(len(classes)))  # Class indices\n",
        "\n",
        "# ============== Create Feature Vectors (Number of classes per image) ===============\n",
        "index = [label.stem for label in labels]  # Use filenames (without extension) as IDs\n",
        "labels_df = pd.DataFrame([], columns=cls_idx, index=index)\n",
        "\n",
        "for label in labels:\n",
        "    lbl_counter = Counter()\n",
        "\n",
        "    with open(label) as lf:\n",
        "        lines = lf.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        lbl_counter[int(line.split(\" \")[0])] += 1  # Extract YOLO class index\n",
        "\n",
        "    labels_df.loc[label.stem] = lbl_counter\n",
        "\n",
        "labels_df = labels_df.fillna(0).infer_objects(copy=False).astype(int)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class_instances = labels_df.sum(axis=0)\n",
        "class_names = [classes[idx] for idx in class_instances.index]\n",
        "\n",
        "# Plot horizontal bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(class_names, class_instances, color=\"skyblue\")\n",
        "plt.xlabel(\"Number of Instances\")\n",
        "plt.ylabel(\"Class Names\")\n",
        "plt.title(\"Instances per Class\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "sum(class_instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_c-3zbZETne"
      },
      "source": [
        "Now, we can split the dataset using these feature vectors\n",
        "## Split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_size = 0.15\n",
        "test_size = 0.10\n",
        "train_size = 1 - validation_size - test_size\n",
        "\n",
        "# =============== Split Dataset into Train, Validation, and Test ====================\n",
        "def split_dataset(labels_df, dataset_path, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, verbose=True):\n",
        "    \"\"\"\n",
        "    Splits the dataset into train, validation, and test sets based on the provided ratios.\n",
        "    Starting from the classes with less instances and in increasing order to ensure that the split respects the ratio.\n",
        "    \"\"\"\n",
        "    assert train_ratio + val_ratio + test_ratio == 1, \"Ratios must sum to 1\"\n",
        "\n",
        "    # Sort classes by number of instances (ascending order)\n",
        "    sorted_index = class_instances.sort_values().index.tolist()\n",
        "    train_ids, val_ids, test_ids = [], [], []\n",
        "\n",
        "    for cls in sorted_index:\n",
        "        cls_ids = labels_df[labels_df[cls] > 0].index.tolist()\n",
        "        cls_train_ids, cls_temp_ids = train_test_split(cls_ids, train_size=train_ratio, random_state=42)\n",
        "        cls_val_ids, cls_test_ids = train_test_split(cls_temp_ids, train_size=test_ratio / (val_ratio + test_ratio), random_state=42)\n",
        "\n",
        "        train_ids.extend(cls_train_ids)\n",
        "        val_ids.extend(cls_val_ids)\n",
        "        test_ids.extend(cls_test_ids)\n",
        "\n",
        "    # Remove duplicates between train, val, and test sets\n",
        "    # with some order magic to keep ratios\n",
        "    if train_ratio > val_ratio and train_ratio > test_ratio:\n",
        "        train_ids = list(set(train_ids) - set(val_ids))\n",
        "        val_ids = list(set(val_ids) - set(train_ids) - set(test_ids))\n",
        "        test_ids = list(set(test_ids) - set(train_ids) - set(val_ids))\n",
        "    else:\n",
        "        val_ids = list(set(val_ids) - set(train_ids))\n",
        "        train_ids = list(set(train_ids) - set(val_ids) - set(test_ids))\n",
        "        test_ids = list(set(test_ids) - set(train_ids) - set(val_ids))\n",
        "\n",
        "    if verbose:\n",
        "        train_instances = labels_df.loc[train_ids,:].sum().sum()\n",
        "        val_instances = labels_df.loc[val_ids,:].sum().sum()\n",
        "        test_instances = labels_df.loc[test_ids,:].sum().sum()\n",
        "        total_instances = train_instances + val_instances + test_instances\n",
        "        total_train = len(train_ids)\n",
        "        total_val = len(val_ids)\n",
        "        total_test = len(test_ids)\n",
        "        total = total_train + total_val + total_test\n",
        "\n",
        "        print(\"\\nWanted split:\")\n",
        "        print(f\"Train samples: {total * train_ratio} ({train_ratio:.2%})\")\n",
        "        print(f\"Validation samples: {total * val_ratio} ({val_ratio:.2%})\")\n",
        "        print(f\"Test samples: {total * test_ratio} ({test_ratio:.2%})\")\n",
        "\n",
        "        # Distribution of instances\n",
        "        print(\"\\nInstance distribution per set:\")\n",
        "        print(f\"Train instances: {train_instances} ({train_instances / total_instances:.2%})\")\n",
        "        print(f\"Validation instances: {val_instances} ({val_instances / total_instances:.2%})\")\n",
        "        print(f\"Test instances: {test_instances} ({test_instances / total_instances:.2%})\")\n",
        "\n",
        "        # Distribution of images\n",
        "        print(f\"\\nTotal images: {total}\")\n",
        "        print(\"\\nImage split:\")\n",
        "        print(f\"Train samples: {total_train} ({total_train / total:.2%})\")\n",
        "        print(f\"Validation samples: {total_val} ({total_val / total:.2%})\")\n",
        "        print(f\"Test samples: {total_test} ({total_test / total:.2%})\")\n",
        "\n",
        "    return train_ids, val_ids, test_ids\n",
        "\n",
        "train_files, val_files, test_files = split_dataset(labels_df, dataset_path,train_ratio=train_size,val_ratio=validation_size,test_ratio=test_size,verbose=True)\n"
      ],
      "metadata": {
        "id": "bUJzy8t59CIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================= Create output folder =============================\n",
        "output_path = Path(\"split_dataset\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    (output_path / split / \"images\").mkdir(parents=True, exist_ok=True)\n",
        "    (output_path / split / \"labels\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def move_files(files, split):\n",
        "    for file in files:\n",
        "        src_img = dataset_path / f\"images/{file}.png\"\n",
        "        src_lbl = dataset_path / f\"labels/{file}.txt\"\n",
        "\n",
        "        dst_img = output_path / split / \"images\" / f\"{file}.png\"\n",
        "        dst_lbl = output_path / split / \"labels\" / f\"{file}.txt\"\n",
        "\n",
        "        if src_img.exists():\n",
        "            shutil.copy(src_img, dst_img)\n",
        "        if src_lbl.exists():\n",
        "            shutil.copy(src_lbl, dst_lbl)\n",
        "\n",
        "move_files(train_files, \"train\")\n",
        "move_files(val_files, \"val\")\n",
        "move_files(test_files, \"test\")\n",
        "\n",
        "# ======================= Create data_split.yaml =============================\n",
        "yaml_content = {\n",
        "    \"train\": str(\"train\"),\n",
        "    \"val\": str(\"val\"),\n",
        "    \"test\": str(\"test\"),\n",
        "    \"nc\": len(classes),\n",
        "    \"names\": classes,\n",
        "}\n",
        "\n",
        "with open(output_path / \"data_split.yaml\", \"w\") as f:\n",
        "    yaml.dump(yaml_content, f)\n",
        "\n",
        "print(\"Dataset successfully split into train, val, and test with stratified distribution.\")"
      ],
      "metadata": {
        "id": "W5B_oeE9L-FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-y6NgbSMbXq"
      },
      "source": [
        "## Split distributions\n",
        "\n",
        "Is our split good ? Let's see that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17IV39fhMbXq"
      },
      "outputs": [],
      "source": [
        "# ===================== Plot Split Distributions =====================\n",
        "import numpy as np\n",
        "\n",
        "# Function to compute class densities (proportions)\n",
        "def compute_class_densities(file_list):\n",
        "    subset_df = labels_df.loc[file_list]  # Filter dataset for the split\n",
        "    class_counts = subset_df.sum(axis=0)  # Get total instances per class\n",
        "    total_instances = labels_df.sum(axis=0)  # Sum across all classes\n",
        "\n",
        "    class_densities = np.zeros(len(classes))\n",
        "    for i in range(len(classes)):\n",
        "        if total_instances[i] != 0: # Avoid 0-Div\n",
        "            class_densities[i] = class_counts[i] / total_instances[i]\n",
        "        else:\n",
        "            class_densities[i] = 0\n",
        "    return class_densities\n",
        "\n",
        "# Compute densities for each split\n",
        "train_densities = compute_class_densities(train_files)\n",
        "val_densities = compute_class_densities(val_files)\n",
        "test_densities = compute_class_densities(test_files)\n",
        "\n",
        "# Create a horizontal bar chart with normalized densities\n",
        "y = np.arange(len(classes))  # Class indices\n",
        "width = 0.25  # Bar width\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot the bars horizontally\n",
        "plt.barh(y - width, train_densities, height=width, label=\"Train\", color=\"blue\", alpha=0.7)\n",
        "plt.barh(y, val_densities, height=width, label=\"Validation\", color=\"orange\", alpha=0.7)\n",
        "plt.barh(y + width, test_densities, height=width, label=\"Test\", color=\"green\", alpha=0.7)\n",
        "\n",
        "# Add labels and title\n",
        "plt.ylabel(\"Class Names\")\n",
        "plt.xlabel(\"Proportion\")\n",
        "plt.title(\"Normalized Class Distribution in Train, Validation, and Test Sets\")\n",
        "plt.yticks(ticks=y, labels=classes)  # Set class names on the y-axis\n",
        "plt.legend()\n",
        "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZswgJQ1MbXq"
      },
      "source": [
        "# Yolo call for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0QYOQK0EY91"
      },
      "outputs": [],
      "source": [
        "!yolo detect train data=/content/split_dataset/data_split.yaml model=yolo11n.pt epochs=20 imgsz=640"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yKEV0EWFdGt"
      },
      "source": [
        "Testing with the validation folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yBhUvqqFi_1"
      },
      "outputs": [],
      "source": [
        "!yolo detect predict model=runs/detect/train/weights/best.pt source=/content/split_dataset/test/images save=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P20mQm6MFus3"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "for image_path in glob.glob(f'/content/runs/detect/predict/*.jpg')[:10]:\n",
        "  display(Image(filename=image_path, height=400))\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the best weights"
      ],
      "metadata": {
        "id": "TKjwnZoB1HW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "!zip -r train.zip runs/detect/train/\n",
        "files.download('train.zip')"
      ],
      "metadata": {
        "id": "bOVpC37k1MIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use the **best.pt** file to run the model in real time."
      ],
      "metadata": {
        "id": "ZcT0jVE7O2Dh"
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}